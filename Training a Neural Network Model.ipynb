{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'ML_data'\n",
    "prefix = 'sagemaker/ML'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "input_data = sagemaker_session.upload_data(path = data_dir, bucket = bucket, key_prefix = prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-001-4bc8c79b/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-002-3dda3937/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-003-b53854c2/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-004-aa39f51b/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-005-24c675e0/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-006-de089ddf/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-007-edf0fee2/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-008-aa577a27/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-009-d79f5982/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-010-db644b5d/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-011-8f94efef/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-012-7d6c52f3/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-013-1c9d34a9/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-014-c5ab4de0/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-015-729ff37c/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-016-d4cf40be/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-017-9f1df5ad/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-018-83e678d1/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-019-6468efaf/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/output/xgboost-200606-2327-020-7662f1d1/output/model.tar.gz\n",
      "sagemaker/ML-tuning-HL/test_x.csv\n",
      "sagemaker/ML-tuning-HL/train.csv\n",
      "sagemaker/ML-tuning-HL/val.csv\n",
      "sagemaker/ML/output/xgboost-2020-06-14-07-08-42-089/output/model.tar.gz\n",
      "sagemaker/ML/std.csv\n",
      "sagemaker/ML/test.csv\n",
      "sagemaker/ML/test_offer_id.csv\n",
      "sagemaker/ML/test_x.csv\n",
      "sagemaker/ML/test_x.csv.out\n",
      "sagemaker/ML/train.csv\n",
      "sagemaker/ML/val.csv\n",
      "xgboost-200606-2327-020-7662f1d1-2020-06-06-23-53-44-403/test_x.csv.out\n",
      "xgboost-200606-2327-020-7662f1d1-2020-06-07-00-46-19-444/test_x.csv.out\n",
      "xgboost-2020-06-14-07-11-54-785/test_x.csv.out\n"
     ]
    }
   ],
   "source": [
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# imports the model in model.py by name\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BinaryClassifier\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = BinaryClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33minput_features\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Load the stored model parameters.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    \u001b[37m# set to eval mode, could use no_grad\u001b[39;49;00m\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[37m# Gets training data in batches from the train.csv file\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir):\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    train_data = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), header=\u001b[36mNone\u001b[39;49;00m, names=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "\r\n",
      "    train_y = torch.from_numpy(train_data[[\u001b[34m0\u001b[39;49;00m]].values).float().squeeze()\r\n",
      "    train_x = torch.from_numpy(train_data.drop([\u001b[34m0\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m).values).float()\r\n",
      "\r\n",
      "    train_ds = torch.utils.data.TensorDataset(train_x, train_y)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(train_ds, batch_size=batch_size)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m# Provided training function\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(model, train_loader, epochs, criterion, optimizer, device):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    This is the training method that is called by the PyTorch training script. The parameters\u001b[39;49;00m\r\n",
      "\u001b[33m    passed are as follows:\u001b[39;49;00m\r\n",
      "\u001b[33m    model        - The PyTorch model that we wish to train.\u001b[39;49;00m\r\n",
      "\u001b[33m    train_loader - The PyTorch DataLoader that should be used during training.\u001b[39;49;00m\r\n",
      "\u001b[33m    epochs       - The total number of epochs to train for.\u001b[39;49;00m\r\n",
      "\u001b[33m    criterion    - The loss function used for training. \u001b[39;49;00m\r\n",
      "\u001b[33m    optimizer    - The optimizer to use during training.\u001b[39;49;00m\r\n",
      "\u001b[33m    device       - Where the model and data should be loaded (gpu or cpu).\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[37m# training loop is provided\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, epochs + \u001b[34m1\u001b[39;49;00m):\r\n",
      "        model.train() \u001b[37m# Make sure that the model is in training mode.\u001b[39;49;00m\r\n",
      "\r\n",
      "        total_loss = \u001b[34m0\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m train_loader:\r\n",
      "            \u001b[37m# get data\u001b[39;49;00m\r\n",
      "            batch_x, batch_y = batch\r\n",
      "\r\n",
      "            batch_x = batch_x.to(device)\r\n",
      "            batch_y = batch_y.to(device)\r\n",
      "\r\n",
      "            optimizer.zero_grad()\r\n",
      "\r\n",
      "            \u001b[37m# get predictions from model\u001b[39;49;00m\r\n",
      "            y_pred = model(batch_x)\r\n",
      "            \r\n",
      "            \u001b[37m# perform backprop\u001b[39;49;00m\r\n",
      "            loss = criterion(y_pred, batch_y)\r\n",
      "            loss.backward()\r\n",
      "            optimizer.step()\r\n",
      "            \r\n",
      "            total_loss += loss.data.item()\r\n",
      "\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch: {}, Loss: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch, total_loss / \u001b[36mlen\u001b[39;49;00m(train_loader)))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m## TODO: Complete the main code\u001b[39;49;00m\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    \r\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\r\n",
      "    \u001b[37m# when this script is executed, during a training job\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models; set automatically\u001b[39;49;00m\r\n",
      "    \u001b[37m# Do not need to change\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \r\n",
      "    \u001b[37m# Training Parameters, given\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 500)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m## TODO: Add args for the three model parameters: input_features, hidden_dim, output_dim\u001b[39;49;00m\r\n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input_features\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of input features to model (default: 3)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m20\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mH\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhidden dim of model (default: 20)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mOUT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33moutput dim of model (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \r\n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing device {}.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\r\n",
      "\r\n",
      "    torch.manual_seed(args.seed)\r\n",
      "\r\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\r\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir)\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m## --- Your code here --- ##\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[37m## TODO:  Build the model by passing in the input params\u001b[39;49;00m\r\n",
      "    \u001b[37m# To get params from the parser, call args.argument_name, ex. args.epochs or ards.hidden_dim\u001b[39;49;00m\r\n",
      "    \u001b[37m# Don't forget to move your model .to(device) to move to GPU , if appropriate\u001b[39;49;00m\r\n",
      "    model = BinaryClassifier(args.input_features, args.hidden_dim, args.output_dim).to(device)\r\n",
      "\r\n",
      "    \u001b[37m## TODO: Define an optimizer and loss function for training\u001b[39;49;00m\r\n",
      "    optimizer = optim.Adam(model.parameters(), lr = \u001b[34m0.0001\u001b[39;49;00m)\r\n",
      "    criterion = torch.nn.BCELoss()\r\n",
      "\r\n",
      "    \u001b[37m# Trains the model (given line of code, which calls the above training function)\u001b[39;49;00m\r\n",
      "    train(model, train_loader, args.epochs, criterion, optimizer, device)\r\n",
      "\r\n",
      "    \u001b[37m## TODO: complete in the model_info by adding three argument names, the first is given\u001b[39;49;00m\r\n",
      "    \u001b[37m# Keep the keys of this dictionary as they are \u001b[39;49;00m\r\n",
      "    model_info_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33minput_features\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.input_features,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.hidden_dim,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33moutput_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.output_dim,\r\n",
      "        }\r\n",
      "        torch.save(model_info, f)\r\n",
      "        \r\n",
      "    \u001b[37m## --- End of your code  --- ##\u001b[39;49;00m\r\n",
      "    \r\n",
      "\r\n",
      "\t\u001b[37m# Save the model parameters\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(args.model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        torch.save(model.cpu().state_dict(), f)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source_pytorch/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                   source_dir='source_pytorch',\n",
    "                   role=role,\n",
    "                   framework_version='1.0',\n",
    "                   train_instance_count=1,\n",
    "                   train_instance_type='ml.p2.xlarge',\n",
    "                   output_path=output_path,\n",
    "                   sagemaker_session=sagemaker_session,\n",
    "                   hyperparameters={\n",
    "                       'input_features':18,\n",
    "                       'hidden_dim':300,\n",
    "                       'output_dim':1,\n",
    "                       'epochs':30,\n",
    "                       'batch-size':1000,\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-14 07:38:18 Starting - Starting the training job...\n",
      "2020-06-14 07:38:20 Starting - Launching requested ML instances.........\n",
      "2020-06-14 07:40:01 Starting - Preparing the instances for training......\n",
      "2020-06-14 07:41:11 Downloading - Downloading input data...\n",
      "2020-06-14 07:41:34 Training - Downloading the training image.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:30,829 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:30,856 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:30,857 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:31,106 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:31,106 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:31,107 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:31,107 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tcb__nxe/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.2b1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:33,300 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 1000,\n",
      "        \"hidden_dim\": 300,\n",
      "        \"input_features\": 18,\n",
      "        \"epochs\": 30,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-06-14-07-38-17-738\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-1-105243015009/sagemaker-pytorch-2020-06-14-07-38-17-738/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":1000,\"epochs\":30,\"hidden_dim\":300,\"input_features\":18,\"output_dim\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-1-105243015009/sagemaker-pytorch-2020-06-14-07-38-17-738/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":1000,\"epochs\":30,\"hidden_dim\":300,\"input_features\":18,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2020-06-14-07-38-17-738\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-1-105243015009/sagemaker-pytorch-2020-06-14-07-38-17-738/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"1000\",\"--epochs\",\"30\",\"--hidden_dim\",\"300\",\"--input_features\",\"18\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=1000\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=300\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_FEATURES=18\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=30\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --batch-size 1000 --epochs 30 --hidden_dim 300 --input_features 18 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cuda.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\n",
      "2020-06-14 07:42:29 Training - Training image download completed. Training in progress.\u001b[34mEpoch: 1, Loss: 0.6549008890639904\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 0.6154680016428925\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 0.5953678266946659\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 0.5870311066161754\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 0.5811004361440969\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 0.5771718607392422\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 0.5726566162220267\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 0.5704856459484544\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 0.5661848797354587\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 0.5650178152461385\u001b[0m\n",
      "\u001b[34mEpoch: 11, Loss: 0.562679854936378\u001b[0m\n",
      "\u001b[34mEpoch: 12, Loss: 0.5615445860596591\u001b[0m\n",
      "\u001b[34mEpoch: 13, Loss: 0.5594638766244401\u001b[0m\n",
      "\u001b[34mEpoch: 14, Loss: 0.5599403755609379\u001b[0m\n",
      "\u001b[34mEpoch: 15, Loss: 0.5580939068350681\u001b[0m\n",
      "\u001b[34mEpoch: 16, Loss: 0.5577642210694247\u001b[0m\n",
      "\u001b[34mEpoch: 17, Loss: 0.5567243237828099\u001b[0m\n",
      "\u001b[34mEpoch: 18, Loss: 0.5565843360368595\u001b[0m\n",
      "\u001b[34mEpoch: 19, Loss: 0.55616295337677\u001b[0m\n",
      "\u001b[34mEpoch: 20, Loss: 0.5550738392874252\u001b[0m\n",
      "\u001b[34mEpoch: 21, Loss: 0.5551283789235492\u001b[0m\n",
      "\u001b[34mEpoch: 22, Loss: 0.5543149016624274\u001b[0m\n",
      "\u001b[34mEpoch: 23, Loss: 0.5544064031090847\u001b[0m\n",
      "\u001b[34mEpoch: 24, Loss: 0.553528601347014\u001b[0m\n",
      "\u001b[34mEpoch: 25, Loss: 0.5540769266527753\u001b[0m\n",
      "\u001b[34mEpoch: 26, Loss: 0.55404195951861\u001b[0m\n",
      "\u001b[34mEpoch: 27, Loss: 0.5532139844672624\u001b[0m\n",
      "\u001b[34mEpoch: 28, Loss: 0.5523003811059997\u001b[0m\n",
      "\u001b[34mEpoch: 29, Loss: 0.551695256732231\u001b[0m\n",
      "\u001b[34mEpoch: 30, Loss: 0.5519541224767995\u001b[0m\n",
      "\u001b[34m2020-06-14 07:42:50,563 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-06-14 07:43:02 Uploading - Uploading generated training model\n",
      "2020-06-14 07:43:02 Completed - Training job completed\n",
      "Training seconds: 111\n",
      "Billable seconds: 111\n",
      "CPU times: user 647 ms, sys: 36.3 ms, total: 684 ms\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://sagemaker-ap-northeast-1-105243015009/sagemaker/ML/train.csv', content_type='csv')\n",
    "estimator.fit({'train':s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 471 ms, sys: 24 ms, total: 495 ms\n",
      "Wall time: 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                    role=role,\n",
    "                    framework_version='1.0',\n",
    "                    entry_point='predict.py',\n",
    "                    source_dir='source_pytorch')\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_data = pd.read_csv(os.path.join(data_dir, \"test.csv\"), header=None, names=None)\n",
    "\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_preds = predictor.predict(test_x.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score: 0.7126669668317575\n",
      "\n",
      "F1 Score: 0.7791683488090432\n",
      "\n",
      "Precision: 0.7444346484461097\n",
      "\n",
      "Recall: 0.8173018753781004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_y_preds, test_y.values)\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(test_y.values, test_y_preds)\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(test_y.values, test_y_preds, average='binary')\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(test_y.values, test_y_preds, average='binary')\n",
    "print('\\nAccuracy Score:', accuracy)\n",
    "print('\\nF1 Score:', f1)\n",
    "print('\\nPrecision:', precision)\n",
    "print('\\nRecall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ResponseMetadata': {'RequestId': '5B13F44B62D4181A',\n",
       "   'HostId': 'sUVo7B7MNB+5N9bqkBjbQPjmexq+LmRTeF88vz9NgJZ0BG7K3qUXxCoKhGiqUjMBe6t3RN7rj3I=',\n",
       "   'HTTPStatusCode': 200,\n",
       "   'HTTPHeaders': {'x-amz-id-2': 'sUVo7B7MNB+5N9bqkBjbQPjmexq+LmRTeF88vz9NgJZ0BG7K3qUXxCoKhGiqUjMBe6t3RN7rj3I=',\n",
       "    'x-amz-request-id': '5B13F44B62D4181A',\n",
       "    'date': 'Sat, 06 Jun 2020 12:19:09 GMT',\n",
       "    'connection': 'close',\n",
       "    'content-type': 'application/xml',\n",
       "    'transfer-encoding': 'chunked',\n",
       "    'server': 'AmazonS3'},\n",
       "   'RetryAttempts': 0},\n",
       "  'Deleted': [{'Key': 'sagemaker/ML/sagemaker-pytorch-2020-06-06-11-57-04-927/debug-output/training_job_end.ts'},\n",
       "   {'Key': 'sagemaker/ML/test_x.csv.out'},\n",
       "   {'Key': 'sagemaker/ML/train.csv'},\n",
       "   {'Key': 'sagemaker-pytorch-2020-06-06-11-48-31-756/sourcedir.tar.gz'},\n",
       "   {'Key': 'sagemaker/ML/std.csv'},\n",
       "   {'Key': 'sagemaker-pytorch-2020-06-06-11-52-46-361/sourcedir.tar.gz'},\n",
       "   {'Key': 'sagemaker/ML/test.csv'},\n",
       "   {'Key': 'sagemaker/ML/sagemaker-pytorch-2020-06-06-11-57-04-927/output/model.tar.gz'},\n",
       "   {'Key': 'sagemaker/ML/test_offer_id.csv'},\n",
       "   {'Key': 'sagemaker-pytorch-2020-06-06-11-48-41-615/sourcedir.tar.gz'},\n",
       "   {'Key': 'sagemaker-pytorch-2020-06-06-12-01-48-140/sourcedir.tar.gz'},\n",
       "   {'Key': 'sagemaker/ML/test_x.csv'},\n",
       "   {'Key': 'sagemaker/ML/val.csv'},\n",
       "   {'Key': 'sagemaker-pytorch-2020-06-06-11-57-04-927/source/sourcedir.tar.gz'}]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
